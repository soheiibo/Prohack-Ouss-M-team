{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestNew.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASfV6nj1XzJy",
        "colab_type": "text"
      },
      "source": [
        "# **PROHACK2020 , Ouss_M team. **\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQDrC_caLKxV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![](https://raw.githubusercontent.com/ousama-88/prohack_dataset_fIZoqt7/master/0%20(1).jpg)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktkT6vQqCM8z",
        "colab_type": "text"
      },
      "source": [
        "**Our solution to the data science hackathon by McKinsey 'Prohack 2020' , team : Ouss_M , which was ranked 2nd in final leaderboard. There are 3 notebooks in this repo :**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   notebook1 : for data :EDA , the final df is the one we worked with in the second notebook\n",
        "2.   notebook2 : more feature engineering + modeling \" TASK 1 SOLUTION \"\n",
        "3.   notebook3 : \" TASK 2 SOLUTION \" based on the y predicted from task 1 'notebook2' \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuRZWwIGBMiN",
        "colab_type": "text"
      },
      "source": [
        " **Probelm Description**\n",
        ": “Beeep…Beeeep….Beeeep… Hooomans*, are you there?...” This very strange transmission is coming from your narrowband radio signal receiver, pointed towards one of the farthest away galaxies. It’s early morning, you are sitting in your radio observatory high in the mountains. For the last 10 years you’ve been a Chief Data Scientist in one of the best astrophysics research teams in the world. You are enjoying a quiet time with a cup of coffee and reviewing the data reports from last night, when this strange sound arrived. You almost spill your coffee in surprise. “Am I dreaming?” is your first thought as you move closer towards the speaker and listen… “Beep…Beeeep….Beeeep… To all Hooomans who can hear us – we need your help” You lean closer and grab a notebook and a pencil – you don’t really trust computers when it comes to such important tasks as taking notes from a radio transmission. You start recording everything that the strange voice from light years away is saying. “… We need serious Data Science help and we know you Hooomans are the best at it…. We are an intergalactic species which have almost achieved singularity and the highest possible levels of development. We travel fast through space and explore other galaxies” “The only essence that we consume is energy, measured in DSML units…Our populace is widespread and we live across many different star clusters and galaxies. What we need now is to optimize our well-being across all those galaxies… We have a lot of data but our сomputers and methods are too weak – we urgently need your data science knowledge to help us” “Only two steps prevent us from achieving singularity · To understand what makes us better off. Our elders used the composite index to measure our well-being performance, but this knowledge has disappeared in the sands of time. Use our data and train your model to predict this index with the highest possible level of certainty. · To achieve the highest possible level of well-being through optimized allocation of additional energy We have discovered the star of an unusually high energy of 50000 zillion DSML. We have agreed between ourselves that · no one galaxy will consume more than 100 zillion DSML and at least 10% of the total energy will be consumed by galaxies in need with existence expectancy index below 0,7. Think of our galaxies as your “countries” (or how you call them??) and our population as citizens. We have similar healthcare and wellbeing characteristic as you, Hooomans” “We are sending all the data to you right now. Let the data be with you, Hoomans… … …” Transmission suddenly ends. You put your notebook and pencil away and start thinking. You really want to help this species optimize their well-being. You open up Python and upload the dataset from the narrowband radio signal receiver. It will be another great day at the observatory today. ———— * probably intergalactic species meant to say “humans” here but we will never know for sure\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYxO_LomBr0W",
        "colab_type": "text"
      },
      "source": [
        "**Description Data Recieved :**\n",
        "The solutions are evaluated on two criteria: predicted future Index values and allocated energy from a newly discovered star\n",
        "\n",
        "1) Index predictions are evaluated using RMSE metric\n",
        "\n",
        "2) Energy allocation is also evaluated using RMSE metric and has a set of known factors that need to be taken into account.\n",
        "\n",
        "Every galaxy has a certain limited potential for improvement in the index described by the following function:\n",
        "\n",
        "Potential for increase in the Index = -np.log(Index+0.01)+3\n",
        "\n",
        "Likely index increase dependent on potential for improvement and on extra energy availability is described by the following function:\n",
        "\n",
        "Likely increase in the Index = extra energy * Potential for increase in the Index **2 / 1000\n",
        "\n",
        "There are also several constraints:\n",
        "\n",
        "in total there are 50000 zillion DSML available for allocation and no galaxy at a point in time should be allocated more than 100 zillion DSML or less than 0 zillion DSML. Galaxies with low existence expectancy index below 0.7 should be allocated at least 10% of the total energy available in the foreseeable future\n",
        "\n",
        "3) Leaderboard is based on a combined scaled metric:\n",
        "\n",
        "80% prediction task RMSE + 20% optimization task RMSE * lambda where lambda is a normalizing factor\n",
        "\n",
        "4) Leaderboard is 80% public and 20% private\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjLIAtroO8V2",
        "colab_type": "text"
      },
      "source": [
        "# **this is the first notebook , how do we get our preprocessed data **\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5BkluDQ9V9",
        "colab_type": "text"
      },
      "source": [
        "in our approache we aim to get clean scalable data as the real data corresponding to **Human Development Data (1990-2018)** , we have figured out that the data used for the problem mentioned in this hackathon is quite similar to the data collected from the real word . \n",
        "\n",
        "![](https://raw.githubusercontent.com/ousama-88/prohack_dataset_fIZoqt7/master/011.PNG)\n",
        "\n",
        "more details : http://hdr.undp.org/en/data !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hvTer9f5G60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "ef504b16-0a73-4f3d-f113-c69253381a5a"
      },
      "source": [
        "!pip install pygam catboost\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import decomposition\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from scipy.stats import ttest_ind\n",
        "from itertools import combinations\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from scipy.interpolate import interp1d, splrep, splev \n",
        "from scipy.optimize import curve_fit\n",
        "import time\n",
        "import numba\n",
        "from sklearn.svm import SVR\n",
        "from tqdm import tqdm\n",
        "from sklearn.base import clone"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygam\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/be/775033ef08a8945bec6ad7973b161ca909f852442e0d7cfb8d1a214de1ac/pygam-0.8.0-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 2.7MB/s \n",
            "\u001b[?25hCollecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/aa/e61819d04ef2bbee778bf4b3a748db1f3ad23512377e43ecfdc3211437a0/catboost-0.23.2-cp36-none-manylinux1_x86_64.whl (64.8MB)\n",
            "\u001b[K     |████████████████████████████████| 64.8MB 61kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pygam) (1.18.5)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from pygam) (3.38.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pygam) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pygam) (1.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->pygam) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Installing collected packages: pygam, catboost\n",
            "Successfully installed catboost-0.23.2 pygam-0.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVEqQwRxEo4r",
        "colab_type": "text"
      },
      "source": [
        "functions used for data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1DNehCR3-FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentil5 (x):\n",
        "    x = x.dropna()\n",
        "    if x.shape[0] > 0:\n",
        "        return np.round(np.percentile(x, q=5), 3)\n",
        "    else: return 0\n",
        "def percentil95 (x): \n",
        "    x = x.dropna()\n",
        "    if x.shape[0] > 0:\n",
        "        return np.round(np.percentile(x, q=95), 3)\n",
        "    else: return 0\n",
        "\n",
        "def group_stat(df, group, for_stat):\n",
        "    gr = df.groupby(group).agg(\n",
        "        Par_min = (for_stat, 'min'),\n",
        "        Par_quantil1 = (for_stat, percentil5),\n",
        "        Par_median = (for_stat, 'median'),\n",
        "        Par_mean = (for_stat, 'mean'),\n",
        "        Par_quantil3 = (for_stat, percentil95),\n",
        "        Par_max = (for_stat, 'max'),\n",
        "        Par_sum = (for_stat, 'sum'),\n",
        "        Par_count = (for_stat, 'count')).reset_index()\n",
        "    return gr\n",
        "\n",
        "def catBoost_rec(df, params, rec):\n",
        "    model = CatBoostRegressor(iterations = 10000, learning_rate = 0.1, depth = 4,\n",
        "                              custom_metric = 'R2', eval_metric = 'R2', verbose = 200)\n",
        "    dfm = df[params+[rec]]\n",
        "    dfm = dfm.dropna()\n",
        "    X = dfm.drop([rec], axis = 1)\n",
        "    y = dfm[rec]    \n",
        "    X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.15)\n",
        "    eval_pool = Pool(X_eval, y_eval, cat_features = [1])\n",
        "    model.fit(X_train, y_train, cat_features = [1], eval_set=eval_pool, early_stopping_rounds=200)\n",
        "    y_pred = model.predict(df[params])\n",
        "    return y_pred\n",
        "\n",
        "def param_int(df, col):\n",
        "    dfn = df[['galactic year', col]]\n",
        "    dfn = dfn.dropna()\n",
        "    f = interp1d(dfn['galactic year'], dfn[col], kind='linear', \n",
        "                 bounds_error = False, assume_sorted = True, fill_value = (dfn.iloc[0,1], dfn.iloc[-1,1]))\n",
        "    y2 = f(df['galactic year'])\n",
        "    return y2\n",
        "\n",
        "def func(x, a, b):     \n",
        "    return a * x + b\n",
        "\n",
        "def func_pow(x, a, b):     \n",
        "    return 1 / (a + np.exp(-b * x))\n",
        "\n",
        "def exp2(x, a, b, c):     \n",
        "    return a + b * x**c\n",
        "\n",
        "def korrect_y(df, par):\n",
        "    gr = group_stat(df, ['galactic year'], par)\n",
        "    y = gr['Par_mean'].median()\n",
        "    for i in range(gr.shape[0]):\n",
        "        if gr.loc[i, 'Par_mean'] == 0:\n",
        "            koef = 1\n",
        "        else:\n",
        "            koef = y / gr.loc[i, 'Par_mean']\n",
        "        df.loc[df['galactic year'] == gr.iloc[i, 0], par] =\\\n",
        "        df.loc[df['galactic year'] == gr.iloc[i, 0], par] * koef\n",
        "    return df\n",
        "\n",
        "def compute_meta_feature(model, X_train, X_test, y_train, cv, log = 0):\n",
        "    X_meta_train = np.zeros((len(y_train), 1), dtype=np.float32)\n",
        "    splits = cv.split(X_train)\n",
        "    for train_fold_index, predict_fold_index in splits:\n",
        "        X_fold_train, X_fold_predict = X_train[train_fold_index], X_train[predict_fold_index]\n",
        "        y_fold_train = y_train[train_fold_index]\n",
        "        \n",
        "        folded_clf = clone(model)\n",
        "        folded_clf.fit(X_fold_train, y_fold_train, cat_features = [1])\n",
        "        \n",
        "        X_meta_train[predict_fold_index] = folded_clf.predict(X_fold_predict).reshape(-1, 1)\n",
        "    \n",
        "    meta_clf = clone(model)\n",
        "    meta_clf.fit(X_train, y_train, cat_features = [1])\n",
        "    \n",
        "    X_meta_test = meta_clf.predict(X_test).reshape(-1, 1)\n",
        "    if log == 1:\n",
        "        X_meta_train = np.exp(X_meta_train)\n",
        "        X_meta_test = np.exp(X_meta_test)\n",
        "    return X_meta_train, X_meta_test\n",
        "\n",
        "def generate_meta_features(models, X_train, X_test, y_train, cv):\n",
        "   \n",
        "    features = [\n",
        "        compute_meta_feature(model, X_train, X_test, y_train, cv)\n",
        "        for model in tqdm(models)\n",
        "    ]\n",
        "    stacked_features_train = np.hstack([\n",
        "        features_train for features_train, features_test in features\n",
        "    ])\n",
        "\n",
        "    stacked_features_test = np.hstack([\n",
        "        features_test for features_train, features_test in features\n",
        "    ])\n",
        "    \n",
        "    return stacked_features_train, stacked_features_test\n",
        "\n",
        "def compute_metric(clf, X_train, y_train, X_test):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return y_pred\n",
        "\n",
        "def filter_iqr_dist(df, par):\n",
        "    df_log = df[par].dropna()\n",
        "    p125, p25, p50, p75, p875 = df_log.quantile([0.125, 0.25, 0.5, 0.75, 0.875])\n",
        "    iqr = p75 - p25\n",
        "\n",
        "    QS = ((p75 - p50) - (p50 - p25)) / (p75 - p25)\n",
        "    OS = ((p875 - p50) - (p50 - p125)) / (p875 - p125)\n",
        "    mc = (3 * QS + 2 * OS) / 5\n",
        "    if mc >= 0:\n",
        "        l = p25 - 2 * np.exp(-3.5 * mc) * iqr\n",
        "        u = p75 + 2 * np.exp(4 * mc) * iqr\n",
        "    else:\n",
        "        l = p25 - 2 * np.exp(-4 * mc) * iqr\n",
        "        u = p75 + 2 * np.exp(3.5 * mc) * iqr\n",
        "    isn1 = df[df[par] <= l].index\n",
        "    df.loc[isn1, par] = np.nan\n",
        "    isn2 = df[df[par] >= u].index\n",
        "    df.loc[isn2, par] = np.nan\n",
        "    if len(list(isn1)) > 0 or len(list(isn2)) > 0:\n",
        "        filter_iqr(df, par)\n",
        "    return df\n",
        "\n",
        "def filter_iqr(df, par):\n",
        "    p25 = df[par].quantile(0.25)\n",
        "    p75 = df[par].quantile(0.75)\n",
        "    iqr = p75 - p25\n",
        "    df.loc[(df[par] > p75 + 1.5 * iqr) | (df[par] < p25 - 1.5 * iqr), par] = np.nan\n",
        "    return df\n",
        "\n",
        "def movecol(df, cols_to_move=[], ref_col='', place='After'):\n",
        "    \n",
        "    cols = df.columns.tolist()\n",
        "    if place == 'After':\n",
        "        seg1 = cols[:list(cols).index(ref_col) + 1]\n",
        "        seg2 = cols_to_move\n",
        "    if place == 'Before':\n",
        "        seg1 = cols[:list(cols).index(ref_col)]\n",
        "        seg2 = cols_to_move + [ref_col]\n",
        "    \n",
        "    seg1 = [i for i in seg1 if i not in seg2]\n",
        "    seg3 = [i for i in cols if i not in seg1 + seg2]\n",
        "    \n",
        "    return(df[seg1 + seg2 + seg3])\n",
        "\n",
        "def valid_ind(df):\n",
        "    df_train = df[~df['galactic year'].isin([1007012,1008016,1009020,1010025,1011030,1012036,1013042,1014049,1015056])]\n",
        "    df_test = df[df['galactic year'].isin([1007012,1008016,1009020,1010025,1011030,1012036,1013042,1014049,1015056])]\n",
        "    \n",
        "    df_test, df_dop = train_test_split(df_test, test_size=0.3, random_state = 0)\n",
        "    \n",
        "    df_train = df_train.append(df_dop[df_dop['galactic year'] < 1015056])\n",
        "    df_test = df_test.append(df_dop[df_dop['galactic year'] == 1015056])\n",
        "    return (df_train.index), (df_test.index)\n",
        "\n",
        "def fill_by_RMSE(dft, df_stats_gal, par = 'y_cor', R2 = 0.97, p_val = 0.5, RMSE = 0.01, n = 3):\n",
        "    ##Select the most similar and create a dictionary galaxy: similar galaxies\n",
        "    df_stats_gal_f = df_stats_gal[(df_stats_gal['R2'] > R2) & (df_stats_gal['p_value'] > p_val) & (df_stats_gal['RMSE'] < RMSE)]\n",
        "    galaxy_comb = dict()\n",
        "    for i in range(df_stats_gal_f.shape[0]):\n",
        "        if df_stats_gal_f.iloc[i, 0] in galaxy_comb:\n",
        "            galaxy_comb[df_stats_gal_f.iloc[i, 0]].append([df_stats_gal_f.iloc[i, 1], df_stats_gal_f.iloc[i, 2]])\n",
        "        else:\n",
        "            galaxy_comb[df_stats_gal_f.iloc[i, 0]] = [[df_stats_gal_f.iloc[i, 1], df_stats_gal_f.iloc[i, 2]]]\n",
        "    \n",
        "    for i in range(df_stats_gal_f.shape[0]):\n",
        "        if df_stats_gal_f.iloc[i, 1] in galaxy_comb:\n",
        "            galaxy_comb[df_stats_gal_f.iloc[i, 1]].append([df_stats_gal_f.iloc[i, 0], df_stats_gal_f.iloc[i, 2]])\n",
        "        else:\n",
        "            galaxy_comb[df_stats_gal_f.iloc[i, 1]] = [[df_stats_gal_f.iloc[i, 0], df_stats_gal_f.iloc[i, 2]]]    \n",
        "    \n",
        "    ##We select only those where more than 3 similar\n",
        "    key_to_del = []\n",
        "    for key in galaxy_comb:\n",
        "        if len(galaxy_comb[key]) < n:\n",
        "            key_to_del.append(key)\n",
        "    for key in key_to_del:\n",
        "        galaxy_comb.pop(key, None)\n",
        "    \n",
        "    ##We fill in RMSE-weighted parameters based on the dictionary on the fact variable\n",
        "    print(dft[par].isnull().sum())\n",
        "    for galaxy in galaxy_comb:\n",
        "        data = dft.loc[dft['galaxy'] == galaxy, ['galactic year', par]]\n",
        "        ind = data.index\n",
        "        sum_error = []\n",
        "        i2 = 0\n",
        "        for galaxy2 in galaxy_comb[galaxy]:\n",
        "            data = pd.merge(data, dft.loc[dft['galaxy'] == galaxy2[0], ['galactic year', par]], \n",
        "                            on=['galactic year'], how='left', suffixes = (str(i2 * 2), str(i2 * 2 + 1)))\n",
        "            data.iloc[:, -1] = data.iloc[:, -1] / galaxy2[1]\n",
        "            sum_error.append(1 / galaxy2[1])\n",
        "            i2 += 1\n",
        "        data['mean'] = data.apply(lambda x: x[2:].sum() / np.sum(x[2:]/x[2:]*sum_error) , axis = 1)\n",
        "        data.index = ind\n",
        "        isn = data.iloc[:, 1]\n",
        "        isn = isn[(isn.isnull()) & (data.iloc[:, 0] >=1011030)].index\n",
        "        dft.loc[isn, par] = data.loc[isn, 'mean']\n",
        "    print(dft[par].isnull().sum())\n",
        "    return dft"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQPf_iJ_vXAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('https://raw.githubusercontent.com/ousama-88/prohack_dataset_fIZoqt7/master/train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/ousama-88/prohack_dataset_fIZoqt7/master/test.csv')\n",
        "\n",
        "train = train[train['galaxy'] != 'NGC 5253']\n",
        "\n",
        "test_years = test['galactic year'].value_counts()\n",
        "test['nulls'] = test.isnull().sum(axis = 1)\n",
        "gr_test = test.groupby('galactic year')['nulls'].mean()\n",
        "test = test.drop(['nulls'], axis = 1)\n",
        "\n",
        "train['nulls'] = train.isnull().sum(axis = 1)\n",
        "gr_train = train.groupby('galactic year')['nulls'].mean()\n",
        "train = train.drop(['nulls'], axis = 1)\n",
        "\n",
        "gr2 = group_stat(train, ['galaxy'], 'y')\n",
        "\n",
        "gr = group_stat(train, ['galactic year'], 'y')\n",
        "\n",
        "df_galaxy = train['galaxy'].value_counts()\n",
        "galaxys = list(df_galaxy.index)\n",
        "cols = list(train.columns)\n",
        "df_years = train['galactic year'].value_counts()\n",
        "years = list(df_years.index)\n",
        "years.sort()\n",
        "    \n",
        "test['y'] = 0\n",
        "test = test.append(train, sort=False).reset_index(drop=True)\n",
        "\n",
        "test.loc[test['y'] == 0, 'y'] = np.nan\n",
        "gr = test.groupby('galaxy').mean()\n",
        "\n",
        "test['y'] = np.log(test['y'])\n",
        "\n",
        "\n",
        "test['Population using at least basic drinking-water services (%)'] = test['Population using at least basic drinking-water services (%)'] - 15\n",
        "test.loc[(test['Population using at least basic drinking-water services (%)'] > 100), 'Population using at least basic drinking-water services (%)'] = 100\n",
        "\n",
        "\n",
        "test.loc[test['Gross income per capita'] <= 0, 'Gross income per capita'] = np.nan\n",
        "test['Gross income per capita'] = np.log(test['Gross income per capita'])\n",
        "\n",
        "test['Population using at least basic sanitation services (%)'] = \\\n",
        "test['Population using at least basic sanitation services (%)'].max() - \\\n",
        "test['Population using at least basic sanitation services (%)'] \n",
        "\n",
        "\n",
        "list_log = ['Population using at least basic sanitation services (%)',\n",
        "            'Mortality rate, under-five (per 1,000 live births)',\n",
        "            'Mortality rate, infant (per 1,000 live births)',\n",
        "            'Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))',\n",
        "            'Adolescent birth rate (births per 1,000 female creatures ages 15-19)',\n",
        "            'Unemployment, total (% of labour force)',\n",
        "            'Unemployment, youth (% ages 15–24)',\n",
        "            'Mortality rate, female grown up (per 1,000 people)',\n",
        "            'Mortality rate, male grown up (per 1,000 people)',\n",
        "            'Infants lacking immunization, red hot disease (% of one-galactic year-olds)',\n",
        "            'Infants lacking immunization, Combination Vaccine (% of one-galactic year-olds)',\n",
        "            'Gross galactic product (GGP) per capita',\n",
        "            'Natural resource depletion',\n",
        "            'Maternal mortality ratio (deaths per 100,000 live births)',\n",
        "            'Estimated gross galactic income per capita, male',\n",
        "            'Estimated gross galactic income per capita, female',\n",
        "            'Domestic credit provided by financial sector (% of GGP)',\n",
        "            'Remittances, inflows (% of GGP)']\n",
        "\n",
        "for par in list_log:\n",
        "    test.loc[test[par] < 0, par] = np.nan\n",
        "test[list_log] = np.log(test[list_log] + 0.1)\n",
        "\n",
        "\n",
        "test.loc[(test['galactic year'] < 1000000), 'Intergalactic Development Index (IDI), Rank'] = \\\n",
        "test.loc[(test['galactic year'] < 1000000), 'Intergalactic Development Index (IDI), Rank'] + 15\n",
        "\n",
        "\n",
        "##Reduction of all parameters to constants (average) by years\n",
        "for col in cols[2:-1]:\n",
        "    test = korrect_y(test, col)\n",
        "\n",
        "test['Population, total calc'] = test['Population, ages 15–64 (millions)'] + test['Population, ages 65 and older (millions)'] + \\\n",
        "test['Population, ages 15–64 (millions)'] * test['Young age (0-14) dependency ratio (per 100 creatures ages 15-64)'] / 100\n",
        "\n",
        "test['total dependency ratio calc'] = test['Population, ages 15–64 (millions)'] / \\\n",
        "(test['Population, total calc'] - test['Population, ages 15–64 (millions)'])\n",
        "\n",
        "test['Gross galactic product (GGP) per capita calc'] = test['Gross galactic product (GGP), total'] / \\\n",
        "test['Population, total calc']\n",
        "\n",
        "test['HDI'] = (test['existence expectancy index'] * test['Income Index'] * test['Education Index'])**(1/3)\n",
        "\n",
        "test['Life expectancy index'] = (test['existence expectancy at birth'] - 20) / (85 - 20)\n",
        "data = test[['Life expectancy index', 'existence expectancy index', 'existence expectancy at birth']]\n",
        "\n",
        "\n",
        "##Gender Ratio Parameters\n",
        "##Parameters of the ratio of indicators in half\n",
        "\n",
        "test['Expected years of education ratio']  = test['Expected years of education, male (galactic years)'] /\\\n",
        "                                            test['Expected years of education, female (galactic years)']\n",
        "test['Population with secondary education ratio']  = test['Population with at least some secondary education, male (% ages 25 and older)'] /\\\n",
        "                                                    test['Population with at least some secondary education, female (% ages 25 and older)']\n",
        "test['Intergalactic Development Index ratio']  = test['Intergalactic Development Index (IDI), male'] /\\\n",
        "                                                test['Intergalactic Development Index (IDI), female']\n",
        "test['Estimated gross galactic income per capita ratio']  = test['Estimated gross galactic income per capita, male'] /\\\n",
        "                                                           test['Estimated gross galactic income per capita, female']\n",
        "test['Intergalactic Development Index ratio, Rank']  = test['Intergalactic Development Index (IDI), male, Rank'] /\\\n",
        "                                                      test['Intergalactic Development Index (IDI), female, Rank']\n",
        "test['Labour force participation rate (% ages 15 and older) ratio']  = test['Labour force participation rate (% ages 15 and older), male'] /\\\n",
        "                                                                      test['Labour force participation rate (% ages 15 and older), female']\n",
        "test['Life expectancy at birth ratio']  = test['Life expectancy at birth, male (galactic years)'] /\\\n",
        "                                         test['Life expectancy at birth, female (galactic years)']\n",
        "test['Mortality rate ratio']  = test['Mortality rate, male grown up (per 1,000 people)'] /\\\n",
        "                               test['Mortality rate, female grown up (per 1,000 people)']\n",
        "test['Expected to mean ratio']  = test['Expected years of education (galactic years)'] / test['Mean years of education (galactic years)']\n",
        "\n",
        "test['Tourists per citizen'] = test['Intergalactic inbound tourists (thousands)'] / test['Population, total calc'] / 1000\n",
        "test['Estimated gross galactic income per capita mean'] = (test['Estimated gross galactic income per capita, female'] + test['Estimated gross galactic income per capita, male']) / 2\n",
        "test['Expected years of education (galactic years) mean'] = (test['Expected years of education, female (galactic years)'] + test['Expected years of education, male (galactic years)']) / 2\n",
        "test['Intergalactic Development Index (IDI) mean'] = (test['Intergalactic Development Index (IDI), female'] + test['Intergalactic Development Index (IDI), male']) / 2\n",
        "test['Intergalactic Development Index (IDI) mean, Rank'] = (test['Intergalactic Development Index (IDI), female, Rank'] + test['Intergalactic Development Index (IDI), male, Rank']) / 2\n",
        "test['Labour force participation rate (% ages 15 and older) mean'] = (test['Labour force participation rate (% ages 15 and older), female'] + test['Labour force participation rate (% ages 15 and older), male']) / 2\n",
        "test['Mean years of education (galactic years) mean'] = (test['Mean years of education, female (galactic years)'] + test['Mean years of education, male (galactic years)']) / 2\n",
        "test['Mortality rate mean'] = (test['Mortality rate, male grown up (per 1,000 people)'] + test['Mortality rate, female grown up (per 1,000 people)']) / 2\n",
        "test['Population with at least some secondary education mean'] = (test['Population with at least some secondary education, female (% ages 25 and older)'] + test['Population with at least some secondary education, male (% ages 25 and older)']) / 2\n",
        "test['Private galaxy capital flows (% of GGP) per capita'] = test['Private galaxy capital flows (% of GGP)'] / test['Population, total calc']\n",
        "\n",
        "\n",
        "cols_drop = ['Remittances, inflows (% of GGP)', 'Population, total (millions)', 'Population, total calc', \n",
        "             'Population, ages 65 and older (millions)', 'Intergalactic inbound tourists (thousands)',\n",
        "             'Population, ages 15–64 (millions)', 'Population, under age 5 (millions)', 'existence expectancy at birth',\n",
        "             'Estimated gross galactic income per capita, female', 'Estimated gross galactic income per capita, male',\n",
        "             'Expected years of education, female (galactic years)', 'Expected years of education, male (galactic years)',\n",
        "             'Intergalactic Development Index (IDI), female', 'Intergalactic Development Index (IDI), male',\n",
        "             'Intergalactic Development Index (IDI), female, Rank', 'Intergalactic Development Index (IDI), male, Rank',\n",
        "             'Mean years of education, female (galactic years)', 'Mean years of education, male (galactic years)',\n",
        "             'Mortality rate, male grown up (per 1,000 people)', 'Mortality rate, female grown up (per 1,000 people)',\n",
        "             'Population with at least some secondary education, female (% ages 25 and older)', \n",
        "             'Population with at least some secondary education, male (% ages 25 and older)',\n",
        "             'Labour force participation rate (% ages 15 and older), female', 'Labour force participation rate (% ages 15 and older), male',\n",
        "             'Outer Galaxies direct investment, net inflows (% of GGP)', 'Jungle area (% of total land area)',\n",
        "             'Gross galactic product (GGP), total', 'Gross capital formation (% of GGP)', 'Total unemployment rate (female to male ratio)',\n",
        "             'Gross fixed capital formation (% of GGP)','Gross enrolment ratio, primary (% of primary under-age population)',\n",
        "             'Renewable energy consumption (% of total final energy consumption)', 'Private galaxy capital flows (% of GGP)']\n",
        "test = test.drop(cols_drop, axis = 1)\n",
        "\n",
        "test = movecol(test, cols_to_move=['y'], ref_col='Private galaxy capital flows (% of GGP) per capita', place='After')\n",
        "cols = list(test.columns)\n",
        "dft_galaxy = group_stat(test, ['galaxy'], 'y').sort_values(by = 'Par_mean')\n",
        "galaxys = dft_galaxy['galaxy'].tolist()\n",
        "\n",
        "scalerx = MinMaxScaler(feature_range = (1, 2))\n",
        "scalery = MinMaxScaler(feature_range = (1, 2))\n",
        "\n",
        "test['galactic year sc'] = scalerx.fit_transform(test['galactic year'].values.reshape(-1, 1))\n",
        "test['y_start'] = 0\n",
        "test['y_din'] = 0\n",
        "\n",
        "for galaxy in galaxys:\n",
        "    dfg = test.loc[test['galaxy'] == galaxy] \n",
        "    dfg = dfg.dropna(subset = ['y'])\n",
        "    gy = dfg[dfg['galactic year'] < 1011030]['galactic year sc'].values\n",
        "    y_din = dfg[dfg['galactic year'] < 1011030]['y'].values\n",
        "    popt, pcov = curve_fit(func, gy, y_din, maxfev = 100000)    \n",
        "    test.loc[test['galaxy'] == galaxy, 'y_din'] = popt[0]    \n",
        "    test.loc[test['galaxy'] == galaxy, 'y_start'] = popt[1]\n",
        "test = test.drop('galactic year sc', axis = 1)\n",
        "\n",
        "i2 = 0\n",
        "model = CatBoostRegressor(iterations = 10000,learning_rate = 0.05, depth = 4,\n",
        "                                           l2_leaf_reg = 0.1, custom_metric = 'RMSE', eval_metric = 'RMSE', verbose = False)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(4)\n",
        "test = test.sort_values(by = 'galactic year')\n",
        "start = time.monotonic()\n",
        "n = 4\n",
        "for par in cols[2:-1]:\n",
        "#    print(par, 'before', np.corrcoef(dft.loc[(~dft['y'].isnull()) & (~dft[par].isnull()), par], dft.loc[(~dft['y'].isnull()) & (~dft[par].isnull()), 'y'])[0, 1])\n",
        "#    fig, ax = plt.subplots(figsize=(20, 20))\n",
        "#    sns.distplot(dft.loc[~dft[par].isnull(), par], hist = True, fit=norm, kde = False, ax=ax)\n",
        "    list_x = ['galactic year', 'Par1', 'y_start', 'y_din']\n",
        "    test['Par1'] = np.random.randint(0, 10000, test.shape[0])\n",
        "    data = test\n",
        "    data = data.dropna(subset = [par])\n",
        "    x = poly.fit_transform(test[list_x])\n",
        "    y = test[par].copy()\n",
        "\n",
        "    xa_sc = scalerx.fit_transform(x)\n",
        "    x_sc = scalerx.transform(poly.transform(data[list_x]))\n",
        "    y_sc = scalery.fit_transform(data[par].values.reshape(-1, 1))\n",
        "    y_sc_r = y_sc.ravel()\n",
        "    \n",
        "    y_pred = np.zeros(xa_sc.shape[0])\n",
        "    cv = KFold(n_splits=n, shuffle=True)\n",
        "    splits = cv.split(x_sc)\n",
        "    for train_fold_index, predict_fold_index in splits:\n",
        "        eval_pool = Pool(x_sc[predict_fold_index], y_sc_r[predict_fold_index])\n",
        "        model.fit(x_sc[train_fold_index], y_sc_r[train_fold_index], eval_set=eval_pool, early_stopping_rounds=50)\n",
        "        y_pred += scalery.inverse_transform(model.predict(xa_sc).reshape(-1, 1)).ravel()\n",
        "   \n",
        "    \n",
        "    y_pred = y_pred / n\n",
        "    \n",
        "    \n",
        "    test[par] = y_pred\n",
        "    test['par'] = y\n",
        "    \n",
        "    for galaxy in galaxys:\n",
        "        spl = splrep(test[test['galaxy'] == galaxy]['galactic year'],test[test['galaxy'] == galaxy][par], k = 3, s = 150)\n",
        "        y_pred = splev(test[test['galaxy'] == galaxy]['galactic year'], spl)\n",
        "        test.loc[test['galaxy'] == galaxy, par] = y_pred \n",
        "end = time.monotonic()\n",
        "test = test.sort_index() \n",
        "\n",
        "test = test.drop(['par', 'Par1', 'y_start', 'y_din'], axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkzB8c115dEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['break_year'] = test['galactic year'].apply(lambda x: 0 if x < 1011030 else 1)\n",
        "test = movecol(test, cols_to_move=['y'], ref_col='break_year', place='After')\n",
        "#cor_all = dft.corr()\n",
        "#df = dft.loc[~dft['y'].isnull(), :]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZzXMhEAB5Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv('dft_medium.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyRtwCiKaAYk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}